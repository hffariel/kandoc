[TOC]
#### 7.1.3.1 ALTER DATABASE

**说明**

该语句用于设置指定数据库的属性（仅管理员使用）。

语法：

   1) 设置数据库数据量配额，单位为B/K/KB/M/MB/G/GB/T/TB/P/PB

``` SQL 
ALTER DATABASE db\_name SET DATA QUOTA quota;
```


   2) 重命名数据库
```
       ALTER DATABASE db\_name RENAME new\_db\_name;
```

   3) 设置数据库的副本数量配额
```
       ALTER DATABASE db\_name SET REPLICA QUOTA quota;
```
说明：

重命名数据库后，如需要，请使用 REVOKE 和 GRANT 命令修改相应的用户权限。

   数据库的默认数据量配额为1024GB，默认副本数量配额为1073741824。

  <br>

**示例**

(1) 设置指定数据库数据量配额

`ALTER DATABASE example_db SET DATA QUOTA 10995116277760;`

   上述单位为字节,等价于

`ALTER DATABASE example_db SET DATA QUOTA 10T;`

  

`ALTER DATABASE example_db SET DATA QUOTA 100G;`

  

`ALTER DATABASE example_db SET DATA QUOTA 200M;`

  

(2) 将数据库 example\_db 重命名为 example\_db2

`ALTER DATABASE example_db RENAME example_db2;`

(3) 设定指定数据库副本数量配额

`ALTER DATABASE example_db SET REPLICA QUOTA 102400;`

  <br>

#### 7.1.3.2 ALTER TABLE

**说明**

该语句用于对已有的 table 进行修改。如果没有指定 rollup index，默认操作 base index。

该语句分为三种操作类型： schema change 、rollup 、partition。这三种操作类型不能同时出现在一条 ALTER TABLE 语句中。其中 schema change 和 rollup 是异步操作，任务提交成功则返回。之后可使用 SHOW ALTER 命令查看进度；partition 是同步操作，命令返回表示执行完毕。

<br>

`语法：`

~~~
ALTER TABLE [database.]table
    alter_clause1[, alter_clause2, ...];
~~~

alter\_clause 分为 partition 、rollup、schema change、rename、index 和swap六种。

  <br>

partition 支持如下几种修改方式

1\. 增加分区

`   语法：`

`       ADD PARTITION [IF NOT EXISTS] partition_name`

`       partition_desc ["key"="value"]`

`       [DISTRIBUTED BY HASH (k1[,k2 ...]) [BUCKETS num]]`

`   注意：`

`       1) partition_desc 支持一下两种写法：`

`           * VALUES LESS THAN [MAXVALUE|("value1", ...)]`

`           * VALUES [("value1", ...), ("value1", ...))`

`       1) 分区为左闭右开区间，如果用户仅指定右边界，系统会自动确定左边界`

`       2) 如果没有指定分桶方式，则自动使用建表使用的分桶方式`

`       3) 如指定分桶方式，只能修改分桶数，不可修改分桶方式或分桶列`

`4) ["key"="value"] 部分可以设置分区的一些属性，具体说明见 CREATE TABLE`

`2. 删除分区`

`   语法：`

`       DROP PARTITION [FORCE] [IF EXISTS] partition_name `

`   注意：`

`       1) 使用分区方式的表至少要保留一个分区。`

`2) 执行 DROP PARTITION 一段时间内，可以通过 RECOVER 语句恢复被删除的分区。详见 RECOVER 语句`

`3) 如果执行 DROP PARTITION FORCE，则系统不会检查该分区是否存在未完成的事务，分区将直接被删除并且不能被恢复，一般不建议执行此操作`

`3. 修改分区属性`

`   语法：`

`MODIFY PARTITION p1|(p1[, p2, ...]) SET ("key" = "value", ...)`

`   说明：`

`       1) 当前支持修改分区的下列属性：`

`           - storage_medium`

`           - storage_cooldown_time`

`           - replication_num`

`           — in_memory`

`       2) 对于单分区表，partition_name 同表名。`

`schema change 支持如下几种修改方式：`

`1. 向指定 index 的指定位置添加一列`

`   语法：`

`       ADD COLUMN column_name column_type [KEY | agg_type] [DEFAULT "default_value"]`

`       [AFTER column_name|FIRST]`

`       [TO rollup_index_name]`

`       [PROPERTIES ("key"="value", ...)]`

`   注意：`

`       1) 聚合模型如果增加 value 列，需要指定 agg_type`

`       2) 非聚合模型（如 DUPLICATE KEY）如果增加key列，需要指定KEY关键字`

`       3) 不能在 rollup index 中增加 base index 中已经存在的列`

`           如有需要，可以重新创建一个 rollup index）`

`2. 向指定 index 添加多列`

`   语法：`

`       ADD COLUMN (column_name1 column_type [KEY | agg_type] DEFAULT "default_value", ...)`

`       [TO rollup_index_name]`

`       [PROPERTIES ("key"="value", ...)]`

`   注意：`

`       1) 聚合模型如果增加 value 列，需要指定agg_type`

`       2) 非聚合模型如果增加key列，需要指定KEY关键字`

`       3) 不能在 rollup index 中增加 base index 中已经存在的列`

`       （如有需要，可以重新创建一个 rollup index）`

`3. 从指定 index 中删除一列`

`   语法：`

`       DROP COLUMN column_name`

`       [FROM rollup_index_name]`

`   注意：`

`       1) 不能删除分区列`

`       2) 如果是从 base index 中删除列，则如果 rollup index 中包含该列，也会被删除`

`4. 修改指定 index 的列类型以及列位置`

`   语法：`

`       MODIFY COLUMN column_name column_type [KEY | agg_type] [NULL | NOT NULL] [DEFAULT "default_value"]`

`       [AFTER column_name|FIRST]`

`       [FROM rollup_index_name]`

`       [PROPERTIES ("key"="value", ...)]`

`   注意：`

`       1) 聚合模型如果修改 value 列，需要指定 agg_type`

`       2) 非聚合类型如果修改key列，需要指定KEY关键字`

`       3) 只能修改列的类型，列的其他属性维持原样（即其他属性需在语句中按照原属性显式的写出，参见 example 8）`

`       4) 分区列不能做任何修改`

`       5) 目前支持以下类型的转换（精度损失由用户保证）`

`           TINYINT/SMALLINT/INT/BIGINT 转换成 TINYINT/SMALLINT/INT/BIGINT/DOUBLE。`

`           TINTINT/SMALLINT/INT/BIGINT/LARGEINT/FLOAT/DOUBLE/DECIMAL 转换成 VARCHAR`

`           VARCHAR 支持修改最大长度`

`           VARCHAR 转换成 TINTINT/SMALLINT/INT/BIGINT/LARGEINT/FLOAT/DOUBLE`

`           VARCHAR 转换成 DATE (目前支持"%Y-%m-%d", "%y-%m-%d", "%Y%m%d", "%y%m%d", "%Y/%m/%d, "%y/%m/%d"六种格式化格式)`

`           DATETIME 转换成 DATE(仅保留年-月-日信息, 例如: '2019-12-09 21:47:05' <--> '2019-12-09')`

`           DATE 转换成 DATETIME(时分秒自动补零， 例如: '2019-12-09' <--> '2019-12-09 00:00:00')`

`           FLOAT 转换成 DOUBLE`

`           INT 转换成 DATE (如果INT类型数据不合法则转换失败，原始数据不变)`

`       6) 不支持从NULL转为NOT NULL`

`5. 对指定 index 的列进行重新排序`

`   语法：`

`       ORDER BY (column_name1, column_name2, ...)`

`       [FROM rollup_index_name]`

`       [PROPERTIES ("key"="value", ...)]`

`   注意：`

`       1) index 中的所有列都要写出来`

`       2) value 列在 key 列之后`

`6. 修改table的属性，目前支持修改bloom filter列, colocate_with 属性和dynamic_partition属性，replication_num和default.replication_num属性`

`   语法：`

`       PROPERTIES ("key"="value")`

`   注意：`

`也可以合并到上面的schema change操作中来修改，见下面例子`

`7. 启用批量删除支持`

`   语法：`

`       ENABLE FEATURE "BATCH_DELETE"`

`   注意：`

`       1） 只能用在unique 表`

`       2) 用于旧表支持批量删除功能，新表创建时已经支持`

`8. 启用按照sequence column的值来保证导入顺序的功能`

`   语法:`

`       ENABLE FEATURE "SEQUENCE_LOAD" WITH PROPERTIES ("function_column.sequence_type" = "Date")`

`   注意：`

`       1）只能用在unique表`

`       2) sequence_type用来指定sequence列的类型，可以为整型和时间类型`

`       3) 只支持新导入数据的有序性，历史数据无法更改`

`rename 支持对以下名称进行修改：`

`1. 修改表名`

`   语法：`

`       RENAME new_table_name;`

`2. 修改 rollup index 名称`

`   语法：`

`       RENAME ROLLUP old_rollup_name new_rollup_name;`

`3. 修改 partition 名称`

`   语法：`

`       RENAME PARTITION old_partition_name new_partition_name;`

`bitmap index 支持如下几种修改方式`

`1. 创建bitmap 索引`

`   语法：`

`       ADD INDEX index_name (column [, ...],) [USING BITMAP] [COMMENT 'balabala'];`

`   注意：`

`       1. 目前仅支持bitmap 索引`

`       1. BITMAP 索引仅在单列上创建`

`2. 删除索引`

`   语法：`

`       DROP INDEX index_name；`

<br>

`    swap 支持将两个表原子替换`
`    1. 将两个表原子替换`
`        语法：`
`            SWAP WITH table_name`
  <br>

**示例**

`[table]`

1. 修改表的默认副本数量, 新建分区副本数量默认使用此值：
    ```sql
    ATLER TABLE example_db.my_table
    SET ("default.replication_num" = "2");
    ```
2. 修改**单分区**表的实际副本数量(只限单分区表)：

    ```sql
    ALTER TABLE example_db.my_table
    SET ("replication_num" = "3");
    ```
    修改表的**所有**分区：
    ```sql
    ALTER TABLE example_db.my_table
    MODIFY PARTITION(*)
    SET ("replication_num" = "3");
    ```



`[partition]`

`1. 增加分区, 现有分区 [MIN, 2013-01-01)，增加分区 [2013-01-01, 2014-01-01)，使用默认分桶方式`

`   ALTER TABLE example_db.my_table`

`   ADD PARTITION p1 VALUES LESS THAN ("2014-01-01");`

`2. 增加分区，使用新的分桶数`

`   ALTER TABLE example_db.my_table`

`   ADD PARTITION p1 VALUES LESS THAN ("2015-01-01")`

`   DISTRIBUTED BY HASH(k1) BUCKETS 20;`

`3. 增加分区，使用新的副本数`

`   ALTER TABLE example_db.my_table`

`   ADD PARTITION p1 VALUES LESS THAN ("2015-01-01")`

`   ("replication_num"="1");`

`4. 修改分区副本数`

`   ALTER TABLE example_db.my_table`

`MODIFY PARTITION p1 SET("replication_num"="1");`

`5. 批量修改指定分区`

`   ALTER TABLE example_db.my_table`

`MODIFY PARTITION (p1, p2, p4) SET("in_memory"="true");`

`6. 批量修改所有分区`

`   ALTER TABLE example_db.my_table`

`MODIFY PARTITION (*) SET("storage_medium"="HDD");`

`7. 删除分区`

`   ALTER TABLE example_db.my_table`

`   DROP PARTITION p1;`

`8. 增加一个指定上下界的分区`

`   ALTER TABLE example_db.my_table`

`   ADD PARTITION p1 VALUES [("2014-01-01"), ("2014-02-01"));`

`[rollup]`

`1. 创建 index: example_rollup_index，基于 base index（k1,k2,k3,v1,v2）。列式存储。`

`   ALTER TABLE example_db.my_table`

`   ADD ROLLUP example_rollup_index(k1, k3, v1, v2)`

`   PROPERTIES("storage_type"="column");`

`2. 创建 index: example_rollup_index2，基于 example_rollup_index（k1,k3,v1,v2）`

`   ALTER TABLE example_db.my_table`

`   ADD ROLLUP example_rollup_index2 (k1, v1)`

`   FROM example_rollup_index;`

`3. 创建 index: example_rollup_index3, 基于 base index (k1,k2,k3,v1), 自定义 rollup 超时时间一小时。`

`   ALTER TABLE example_db.my_table`

`   ADD ROLLUP example_rollup_index(k1, k3, v1)`

`   PROPERTIES("storage_type"="column", "timeout" = "3600");`

`4. 删除 index: example_rollup_index2`

`   ALTER TABLE example_db.my_table`

`   DROP ROLLUP example_rollup_index2;`

`[schema change]`

`1. 向 example_rollup_index 的 col1 后添加一个key列 new_col(非聚合模型)`

`   ALTER TABLE example_db.my_table`

`   ADD COLUMN new_col INT KEY DEFAULT "0" AFTER col1`

`   TO example_rollup_index;`

`2. 向example_rollup_index的col1后添加一个value列new_col(非聚合模型)`

`     ALTER TABLE example_db.my_table`

`     ADD COLUMN new_col INT DEFAULT "0" AFTER col1    `

`     TO example_rollup_index;`

`3. 向example_rollup_index的col1后添加一个key列new_col(聚合模型)`

`     ALTER TABLE example_db.my_table`

`     ADD COLUMN new_col INT DEFAULT "0" AFTER col1    `

`     TO example_rollup_index;`

`4. 向example_rollup_index的col1后添加一个value列new_col SUM聚合类型(聚合模型)`

`     ALTER TABLE example_db.my_table`

`     ADD COLUMN new_col INT SUM DEFAULT "0" AFTER col1    `

`     TO example_rollup_index;`

`5. 向 example_rollup_index 添加多列(聚合模型)`

`   ALTER TABLE example_db.my_table`

`   ADD COLUMN (col1 INT DEFAULT "1", col2 FLOAT SUM DEFAULT "2.3")`

`   TO example_rollup_index;`

`6. 从 example_rollup_index 删除一列`

`   ALTER TABLE example_db.my_table`

`   DROP COLUMN col2`

`   FROM example_rollup_index;`

`7. 修改 base index 的 col1 列的类型为 BIGINT，并移动到 col2 列后面`

`   ALTER TABLE example_db.my_table`

`   MODIFY COLUMN col1 BIGINT DEFAULT "1" AFTER col2;`

`8. 修改 base index 的 val1 列最大长度。原 val1 为 (val1 VARCHAR(32) REPLACE DEFAULT "abc")`

`   ALTER TABLE example_db.my_table`

`MODIFY COLUMN val1 VARCHAR(64) REPLACE DEFAULT "abc";`

`9. 重新排序 example_rollup_index 中的列（设原列顺序为：k1,k2,k3,v1,v2）`

`   ALTER TABLE example_db.my_table`

`   ORDER BY (k3,k1,k2,v2,v1)`

`   FROM example_rollup_index;`

`10. 同时执行两种操作`

`   ALTER TABLE example_db.my_table`

`   ADD COLUMN v2 INT MAX DEFAULT "0" AFTER k2 TO example_rollup_index,`

`   ORDER BY (k3,k1,k2,v2,v1) FROM example_rollup_index;`

`11. 修改表的 bloom filter 列`

`   ALTER TABLE example_db.my_table SET ("bloom_filter_columns"="k1,k2,k3");`

`也可以合并到上面的 schema change 操作中（注意多子句的语法有少许区别）`

`   ALTER TABLE example_db.my_table`

`   DROP COLUMN col2`

`   PROPERTIES ("bloom_filter_columns"="k1,k2,k3");`

`12. 修改表的Colocate 属性`

`   ALTER TABLE example_db.my_table set ("colocate_with" = "t1");`

`13. 将表的分桶方式由 Random Distribution 改为 Hash Distribution`

`   ALTER TABLE example_db.my_table set ("distribution_type" = "hash");`

`14. 修改表的动态分区属性(支持未添加动态分区属性的表添加动态分区属性)`

`   ALTER TABLE example_db.my_table set ("dynamic_partition.enable" = "false");`

`   如果需要在未添加动态分区属性的表中添加动态分区属性，则需要指定所有的动态分区属性`

`   ALTER TABLE example_db.my_table set ("dynamic_partition.enable" = "true", "dynamic_partition.time_unit" = "DAY", "dynamic_partition.end" = "3", "dynamic_partition.prefix" = "p", "dynamic_partition.buckets" = "32");`

`15. 修改表的 in_memory 属性`

`   ALTER TABLE example_db.my_table set ("in_memory" = "true");`

`16. 启用 批量删除功能`

`   ALTER TABLE example_db.my_table ENABLE FEATURE "BATCH_DELETE"`

`17. 启用按照sequence column的值来保证导入顺序的功能`

`   ALTER TABLE example_db.my_table ENABLE FEATURE "SEQUENCE_LOAD" WITH PROPERTIES ("function_column.sequence_type" = "Date")`

`[rename]`

`1. 将名为 table1 的表修改为 table2`

`   ALTER TABLE table1 RENAME table2;`

`2. 将表 example_table 中名为 rollup1 的 rollup index 修改为 rollup2`

`   ALTER TABLE example_table RENAME ROLLUP rollup1 rollup2;`

`3. 将表 example_table 中名为 p1 的 partition 修改为 p2`

`   ALTER TABLE example_table RENAME PARTITION p1 p2;`

`[index]`

`1. 在table1 上为siteid 创建bitmap 索引`

`   ALTER TABLE table1 ADD INDEX index_name (siteid) [USING BITMAP] COMMENT 'balabala';`

`2. 删除table1 上的siteid列的bitmap 索引`

`   ALTER TABLE table1 DROP INDEX index_name;`

`[swap]`
    `1. 将table1与table2原子替换`
        `ALTER TABLE table1 SWAP WITH table2`

<br>

**keyword**

    ALTER,TABLE,ROLLUP,COLUMN,PARTITION,RENAME,SWAP

  <br>

#### 7.1.3.3 ALTER VIEW

**说明**

该语句用于修改一个view的定义。

语法：

~~~
ALTER VIEW
    [db_name.]view_name
    (column1[ COMMENT "col comment"][, column2, ...])
    AS query_stmt
~~~

说明：

   1. 视图都是逻辑上的，其中的数据不会存储在物理介质上，在查询时视图将作为语句中的子查询，因此，修改视图的定义等价于修改query\_stmt。

   2. query\_stmt 为任意支持的 SQL

  <br>

**示例**

修改example\_db上的视图example\_view。

~~~
ALTER VIEW example_db.example_view
(
        c1 COMMENT "column 1",
        c2 COMMENT "column 2",
        c3 COMMENT "column 3"
)
AS SELECT k1, k2, SUM(v1) FROM example_table 
GROUP BY k1, k2;
~~~

#### 7.1.3.4 BACKUP

**说明**

该语句用于备份指定数据库下的数据。该命令为异步操作。提交成功后，需通过 SHOW BACKUP 命令查看进度。仅支持备份 OLAP 类型的表。

语法：

~~~
BACKUP SNAPSHOT [db_name].{snapshot_name}
    TO `repository_name`
    ON (
        `table_name` [PARTITION (`p1`, ...)],
        ...
    )
    PROPERTIES ("key"="value", ...);
~~~

说明：
   1.同一数据库下只能有一个正在执行的 BACKUP 或 RESTORE 任务。

   2. ON 子句中标识需要备份的表和分区。如果不指定分区，则默认备份该表的所有分区。

   3. PROPERTIES 目前支持以下属性：

*            "type" = "full"：表示这是一次全量更新（默认）。

*            "timeout" = "3600"：任务超时时间，默认为一天。单位秒。

  

**示例**

`1. 全量备份 example_db 下的表 example_tbl 到仓库 example_repo 中：`

`   BACKUP SNAPSHOT example_db.snapshot_label1`

`   TO example_repo`

`   ON (example_tbl)`

`   PROPERTIES ("type" = "full");`

`2. 全量备份 example_db 下，表 example_tbl 的 p1, p2 分区，以及表 example_tbl2 到仓库 example_repo 中：`

`   BACKUP SNAPSHOT example_db.snapshot_label2`

`   TO example_repo`

`   ON`

`   (`

`       example_tbl PARTITION (p1,p2),`

`       example_tbl2`

`   );`

  <br>

#### 7.1.3.5 CANCEL ALTER

**说明**

该语句用于撤销一个 ALTER 操作。

(1) 撤销 ALTER TABLE COLUMN 操作

语法：

`CANCEL` `ALTER`  `TABLE`  `COLUMN`

`   FROM db_name.table_name`

  

(2) 撤销 ALTER TABLE ROLLUP 操作

语法：

`CANCEL` `ALTER`  `TABLE`  `ROLLUP`

`   FROM db_name.table_name`

  

(3) 根据job id批量撤销rollup操作

语法:

`CANCEL` `ALTER`  `TABLE`  `ROLLUP`

`           FROM db_name.table_name (jobid,...)`

> 注：该命令为异步操作，具体是否执行成功需要使用show alter table rollup查看任务状态确认。

  

**示例**

\[CANCEL ALTER TABLE COLUMN\]

撤销针对 my\_table 的 ALTER COLUMN 操作。

`CANCEL ALTER TABLE COLUMN`

`   FROM example_db.my_table;`

\[CANCEL ALTER TABLE ROLLUP\]

撤销 my\_table 下的 ADD ROLLUP 操作。

`CANCEL ALTER TABLE ROLLUP`

`   FROM example_db.my_table;`

\[CANCEL ALTER TABLE ROLLUP\]

根据job id撤销 my\_table 下的 ADD ROLLUP 操作。

`CANCEL ALTER TABLE ROLLUP`

`    FROM example_db.my_table (12801,12802);`

  <br>

#### 7.1.3.6 CANCEL BACKUP

**说明**

该语句用于取消一个正在进行的 BACKUP 任务。

语法：

~~~
CANCEL BACKUP FROM db_name;
~~~

**示例**

取消 example\_db 下的 BACKUP 任务。

`CANCEL BACKUP FROM example_db;`

  <br>

#### 7.1.3.7 CANCEL RESTORE

**说明**

该语句用于取消一个正在进行的 RESTORE 任务。

`语法：`

`CANCEL RESTORE FROM db_name;`

> 注：当取消处于 COMMIT 或之后阶段的 RESTORE 任务时，可能导致被恢复的表无法访问。此时只能通过再次执行恢复作业进行数据恢复。

  

**示例**

取消 example\_db 下的 RESTORE 任务。

`CANCEL RESTORE FROM example_db;`

  <br>

#### 7.1.3.8 CREATE DATABASE

**说明**

该语句用于新建数据库（database）。

`语法：`

`   CREATE DATABASE [IF NOT EXISTS] db_name;`

  <br>

**示例**

新建数据库 db\_test。

`CREATE DATABASE db_test;`

  <br>

#### 7.1.3.9 CREATE INDEX

**说明**

该语句用于创建索引。

`语法：`

`   CREATE INDEX index_name ON table_name (column [, ...],) [USING BITMAP] [COMMENT'balabala'];`

> 注意： (1) 目前只支持 BITMAP 索引； (2) BITMAP 索引仅在单列上创建。

  

**示例**

在 table1 上为 siteid 创建 BITMAP 索引。

`CREATE INDEX index_name ON table1 (siteid) USING BITMAP COMMENT 'balabala';`

  <br>

#### 7.1.3.10 CREATE MATERIALIZED VIEW

**说明**

该语句用于创建物化视图。该命令**异步执行**，任务提交后，用户需要通过 `show alter table rollup` 来查看物化视图的创建进度。 在显示 FINISHED 后即可通过 `desc [table_name] all`命令来查看物化视图的 schema。

语法：

~~~sql
CREATE MATERIALIZED VIEW [MV_name] as [query]
    [PROPERTIES ("key" = "value")]
~~~

（1）MV_name

物化视图的名称，必填项。相同表的物化视图名称不可重复。

（2）query

用于构建物化视图的查询语句，查询语句的结果既物化视图的数据。目前支持的 query 语句为（语法同普通查询语句）：

~~~sql
SELECT select_expr[, select_expr ...]
    FROM [Base view name]
    GROUP BY column_name[, column_name ...]
    ORDER BY column_name[, column_name ...]
~~~

* select\_expr：物化视图的 schema 中所有的列，并需满足如下约束：

    *   仅支持**不带表达式计算**的单列、聚合列。
    *   聚合函数目前仅支持 SUM, MIN, MAX 三种，且聚合函数的参数只能是不带表达式计算的单列。
    *   至少包含一个单列。
    *   所有涉及到的列，**均只能出现一次**。

*   base view name：物化视图的原始表名，必填项。同时要求是单表，且不是子查询。
*   group by：物化视图的分组列，选填项。不填则数据不进行分组。
*   order by：物化视图的排序列，选填项。

    *   排序列的**声明顺序必须和 select\_expr 中列声明顺序一致**。
    *   如果不声明 order by，则根据规则自动补充排序列。 如果物化视图是聚合类型，则所有的分组列自动补充为排序列。 如果物化视图是非聚合类型，则前 36 个字节自动补充为排序列。如果自动补充的排序个数小于3个，则前三个作为排序列。
    *   如果 query 中包含分组列，则排序列必须和分组列一致。

（3）properties

声明物化视图的一些配置，选填项。

`PROPERTIES ("key" = "value", "key" = "value" ...)`

以下几个配置均可声明在此处。

`short_key: 排序列的个数。`

`timeout: 物化视图构建的超时时间。`

  <br>

**示例**

Base 表结构为：

`mysql> desc duplicate_table;`
```sql
+-------+--------+------+------+---------+-------+
| Field |  Type  | Null |  Key | Default | Extra |
+-------+--------+------+------+---------+-------+
| k1    |   INT  | Yes  | true | N/A     |       |
| k2    |   INT  | Yes  | true | N/A     |       |
| k3    | BIGINT | Yes  | true | N/A     |       |
| k4    | BIGINT | Yes  | true | N/A     |       |
+-------+--------+------+------+---------+-------+
```

（1）创建一个仅包含原始表 （k1, k2）列的物化视图

`create materialized view k1_k2 as`

`   select k1, k2 from duplicate_table;`

物化视图的 schema 如下图，物化视图仅包含两列 k1，k2 且不带任何聚合列。

```sql
+-----------------+-------+--------+------+------+---------+-------+
| IndexName       | Field |  Type  | Null | Key  | Default | Extra |
+-----------------+-------+--------+------+------+---------+-------+
| k1_k2           |   k1  |  INT   | Yes  | true |   N/A   |       |
|                 |   k2  |  INT   | Yes  | true |   N/A   |       |
+-----------------+-------+--------+------+------+---------+-------+
```

  <br>

（2）创建一个以 k2 为排序列的物化视图

`create materialized view k2_order as`

`   select k2, k1 from duplicate_table order by k2;`

物化视图的 schema 如下图，物化视图仅包含两列 k2 和 k1，其中 k2 列为排序列，不带任何聚合。

`+-----------------+-------+--------+------+-------+---------+-------+`

`| IndexName       | Field | Type   | Null | Key   | Default | Extra |`

`+-----------------+-------+--------+------+-------+---------+-------+`

`| k2_order        | k2| INT    | Yes| true | N/A |       |`

`|                 | k1| INT    | Yes| false | N/A | NONE  |`

`+-----------------+-------+--------+------+-------+---------+-------+`

  <br>

（3）创建一个以 k1, k2 分组，k3 列为 SUM 聚合的物化视图

`create materialized view k1_k2_sumk3 as`

`   select k1, k2, sum(k3) from duplicate_table group by k1, k2;`

物化视图的 schema 如下图，物化视图包含三列 k1、k2 和 sum(k3)。 其中 k1, k2 为分组列，sum(k3) 为根据 k1, k2 分组后的 k3 列的求和值。由于物化视图没有声明排序列，且物化视图带聚合数据，系统默认补充分组列 k1, k2 为排序列。

`+-----------------+-------+--------+------+-------+---------+-------+`

`| IndexName       | Field | Type   | Null | Key   | Default | Extra |`

`+-----------------+-------+--------+------+-------+---------+-------+`

`| k1_k2_sumk3     | k1| INT    | Yes| true | N/A |       |`

`|                 | k2| INT    | Yes| true | N/A |       |`

`|                 | k3| BIGINT | Yes| false | N/A | SUM   |`

`+-----------------+-------+--------+------+-------+---------+-------+`

  <br>

（4）创建一个去除重复行的物化视图

`create materialized view deduplicate as`

`   select k1, k2, k3, k4 from duplicate_table group by k1, k2, k3, k4;`

物化视图 schema 如下图，物化视图包含 k1, k2, k3, k4列，且不存在重复行。

`+-----------------+-------+--------+------+-------+---------+-------+`

`| IndexName       | Field | Type   | Null | Key   | Default | Extra |`

`+-----------------+-------+--------+------+-------+---------+-------+`

`| deduplicate     | k1| INT    | Yes| true | N/A |       |`

`|                 | k2| INT    | Yes| true | N/A |       |`

`|                 | k3| BIGINT | Yes| true | N/A |       |`

`|                 | k4| BIGINT | Yes| true | N/A |       |`

`+-----------------+-------+--------+------+-------+---------+-------+`

  <br>

（5）创建一个不声明排序列的非聚合型物化视图

all\_type\_table 的 schema 如下：

`+-------+--------------+------+-------+---------+-------+`

`| Field | Type | Null | Key | Default | Extra |`

`+-------+--------------+------+-------+---------+-------+`

`| k1    | TINYINT| Yes  | true  | N/A     |       |`

`| k2    | SMALLINT | Yes  | true  | N/A     |       |`

`| k3    | INT| Yes  | true  | N/A     |       |`

`| k4    | BIGINT | Yes  | true  | N/A     |       |`

`| k5    | DECIMAL(9,0) | Yes  | true  | N/A     |       |`

`| k6    | DOUBLE | Yes  | false | N/A     | NONE|`

`| k7    | VARCHAR(20)  | Yes  | false | N/A     | NONE|`

`+-------+--------------+------+-------+---------+-------+`

物化视图包含 k3, k4, k5, k6, k7 列，且不声明排序列，则创建语句如下：

`create materialized view mv_1 as`

`   select k3, k4, k5, k6, k7 from all_type_table;`

系统默认补充的排序列为 k3, k4, k5 三列。这三列类型的字节数之和为 4(INT) + 8(BIGINT) + 16(DECIMAL) = 28 < 36。所以补充的是这三列作为排序列。 物化视图的 schema 如下，可以看到其中 k3, k4, k5 列的 key 字段为 true，也就是排序列。k6, k7 列的 key 字段为 false，也就是非排序列。

`+----------------+-------+--------------+------+-------+---------+-------+`

`| IndexName      | Field | Type         | Null | Key   | Default | Extra |`

`+----------------+-------+--------------+------+-------+---------+-------+`

`| mv_1           | k3| INT          | Yes| true | N/A |       |`

`|                | k4| BIGINT       | Yes| true | N/A |       |`

`|                | k5| DECIMAL(9,0) | Yes| true | N/A |       |`

`|                | k6| DOUBLE       | Yes| false | N/A | NONE  |`

`|                | k7| VARCHAR(20)  | Yes| false | N/A | NONE  |`

`+----------------+-------+--------------+------+-------+---------+-------+`

  <br>

#### 7.1.3.11 CREATE REPOSITORY

**说明**

该语句用于创建仓库。仓库用于备份或恢复数据。仅 root 或 superuser 用户可以创建仓库。

语法：

~~~
CREATE [READ ONLY] REPOSITORY `repo_name`
    WITH BROKER `broker_name`
    ON LOCATION `repo_location`
    PROPERTIES ("key"="value", ...);
~~~

说明：

   1. 仓库的创建，依赖于已存在的 broker。

   2. 如果是只读仓库，则只能在仓库上进行恢复。如果不是，则可以进行备份和恢复操作。

   3. 根据 broker 的不同类型，PROPERTIES 有所不同，具体见示例。

  

**示例**

（1）创建名为 bos\_repo 的仓库，依赖 BOS broker "bos\_broker"，数据根目录为：bos://palo\_backup.

`CREATE REPOSITORY 'bos_repo'`

`WITH BROKER 'bos_broker'`

`ON LOCATION "bos://palo_backup"`

`PROPERTIES`

`(`

`   "bos_endpoint" = "[http://gz.bcebos.com](http://gz.bcebos.com)",`

`   "bos_accesskey" = "069fc2786e664e63a5f111111114ddbs22",`

`   "bos_secret_accesskey"="70999999999999de274d59eaa980a"`

`);`

（2）创建和示例 1 相同的仓库，但属性为只读：

`CREATE READ ONLY REPOSITORY 'bos_repo'`

`WITH BROKER 'bos_broker'`

`ON LOCATION "bos://palo_backup"`

`PROPERTIES`

`(`

`   "bos_endpoint" = "[http://gz.bcebos.com](http://gz.bcebos.com)",`

`   "bos_accesskey" = "069fc2786e664e63a5f111111114ddbs22",`

`   "bos_secret_accesskey"="70999999999999de274d59eaa980a"`

`);`

（3）创建名为 hdfs\_repo 的仓库，依赖 Baidu hdfs broker "hdfs\_broker"，数据根目录为：hdfs://hadoop-name-node:54310/path/to/repo/。

`CREATE REPOSITORY 'hdfs_repo'`

`WITH BROKER 'hdfs_broker'`

`ON LOCATION "hdfs://hadoop-name-node:54310/path/to/repo/"`

`PROPERTIES`

`(`

`   "username" = "user",`

`   "password" = "password"`

`);`

#### 7.1.3.12 CREATE RESOURCE

**说明**

该语句用于创建资源。仅 root 或 admin 用户可以创建资源。目前仅支持 Spark 外部资源。

`语法：`

`   CREATE [EXTERNAL] RESOURCE "resource_name"`

`   PROPERTIES ("key"="value", ...);    `

说明：

（1）PROPERTIES中需要指定资源的类型 "type" = "spark"，目前仅支持 spark。

（2）根据资源类型的不同 PROPERTIES 有所不同，具体见示例。

  <br>

**示例**

创建 yarn cluster 模式，名为 spark0 的 Spark 资源。

~~~
CREATE EXTERNAL RESOURCE "spark0"
PROPERTIES
(
  "type" = "spark",
  "spark.master" = "yarn",
  "spark.submit.deployMode" = "cluster",
  "spark.jars" = "xxx.jar,yyy.jar",
  "spark.files" = "/tmp/aaa,/tmp/bbb",
  "spark.executor.memory" = "1g",
  "spark.yarn.queue" = "queue0",
  "spark.hadoop.yarn.resourcemanager.address" = "127.0.0.1:9999",
  "spark.hadoop.fs.defaultFS" = "hdfs://127.0.0.1:10000",
  "working_dir" = "hdfs://127.0.0.1:10000/tmp/doris",
  "broker" = "broker0",
  "broker.username" = "user0",
  "broker.password" = "password0"
);
~~~

Spark 相关参数如下：                                                    

(1) spark.master: 必填，目前支持yarn，spark://host:port。                        

(2) spark.submit.deployMode: Spark 程序的部署模式，必填，支持 cluster，client 两种。

(3) spark.hadoop.yarn.resourcemanager.address: master为yarn时必填。              

(4) spark.hadoop.fs.defaultFS: master为yarn时必填。                              

(5) 其他参数为可选，参考http://spark.apache.org/docs/latest/configuration.html

Spark 用于 ETL 时需要指定 working\_dir 和 broker。说明如下：

*   working\_dir：ETL使用的目录。Spark作为ETL资源使用时必填。例如：hdfs://host:port/tmp/doris .
*   broker：broker 名字。Spark作为ETL资源使用时必填。需要使用ALTER SYSTEM ADD BROKER命令提前完成配置。
*   broker.property\_key：broker读取ETL生成的中间文件时需要指定的认证信息等。

<br>

#### 7.1.3.13 CREATE TABLE

**说明**

该语句用于创建 table。 语法定义如下：

~~~
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [database.]table_name
    (column_definition1[, column_definition2, ...]
    [, index_definition1[, ndex_definition12,]])
    [ENGINE = [olap|mysql|broker|hive]]
    [key_desc]
    [COMMENT "table comment"];
    [partition_desc]
    [distribution_desc]
    [rollup_index]
    [PROPERTIES ("key"="value", ...)]
    [BROKER PROPERTIES ("key"="value", ...)]
~~~

  

**（1）column\_definition**

`语法：`

`   col_name col_type [agg_type] [NULL | NOT NULL] [DEFAULT "default_value"]`

<br>

说明：

*   col\_name：列名称。
*   col\_type：列类型。支持的类型有：

~~~
TINYINT（1字节）
    范围：-2^7 + 1 ~ 2^7 - 1
SMALLINT（2字节）
    范围：-2^15 + 1 ~ 2^15 - 1
INT（4字节）
    范围：-2^31 + 1 ~ 2^31 - 1
BIGINT（8字节）
    范围：-2^63 + 1 ~ 2^63 - 1
LARGEINT（16字节）
    范围：-2^127 + 1 ~ 2^127 - 1
FLOAT（4字节）
    支持科学计数法
DOUBLE（12字节）
    支持科学计数法
DECIMAL[(precision, scale)] (16字节)
    保证精度的小数类型。默认是 DECIMAL(10, 0)
    precision: 1 ~ 27
    scale: 0 ~ 9
    其中整数部分为 1 ~ 18
    不支持科学计数法
DATE（3字节）
    范围：0000-01-01 ~ 9999-12-31
DATETIME（8字节）
    范围：0000-01-01 00:00:00 ~ 9999-12-31 23:59:59
CHAR[(length)]
    定长字符串。长度范围：1 ~ 255。默认为1
VARCHAR[(length)]
    变长字符串。长度范围：1 ~ 65533
HLL (1~16385个字节)
    hll列类型，不需要指定长度和默认值、长度根据数据的聚合
    程度系统内控制，并且HLL列只能通过配套的hll_union_agg、Hll_cardinality、hll_hash进行查询或使用。
BITMAP
    bitmap列类型，不需要指定长度和默认值。表示整型的集合，元素最大支持到（2^64 - 1）。
~~~

*   agg\_type：聚合类型。如果不指定，则该列为 key 列。否则该列为 value 列。支持的聚合类型包括：

    *   SUM、MAX、MIN、REPLACE
    *   HLL\_UNION(仅用于HLL列，为HLL独有的聚合方式)
    *   BITMAP\_UNION(仅用于 BITMAP 列，为 BITMAP 独有的聚合方式)
    *   REPLACE\_IF\_NOT\_NULL：这个聚合类型的含义是当且仅当新导入数据是非NULL值时会发生替换行为，如果新导入的数据是NULL，那么DorisDB仍然会保留原值。注意：如果用在建表时REPLACE\_IF\_NOT\_NULL列指定了NOT NULL，那么DorisDB仍然会将其转化NULL，不会向用户报错。用户可以借助这个类型完成部分列导入的功能。

该类型参数只对聚合模型(key\_desc的type为AGGREGATE KEY)有用，其它模型不需要指定该参数。

  

*   是否允许为NULL：默认不允许为 NULL。NULL 值在导入数据中用 \\N 来表示。

> 注：BITMAP\_UNION聚合类型列在导入时的原始数据类型必须是TINYINT,SMALLINT,INT,BIGINT。

  

**（2）index\_definition**

`语法：`

`INDEX index_name (col_name[, col_name, ...]) [USING BITMAP] COMMENT 'xxxxxx'`

<br>

说明：

*   index\_name：索引名称
*   col\_name：列名

> 注：当前仅支持BITMAP索引， BITMAP索引仅支持应用于单列。

  

**（3）ENGINE 类型**

默认为 olap，可选 mysql, broker, hive。

（a）如果是 mysql，则需要在 properties 提供以下信息：

~~~
PROPERTIES (
    "host" = "mysql_server_host",
    "port" = "mysql_server_port",
    "user" = "your_user_name",
    "password" = "your_password",
    "database" = "database_name",
    "table" = "table_name"
)
~~~

> 注："table" 条目中的 "table\_name" 是 mysql 中的真实表名。而 CREATE TABLE 语句中的 table\_name 是该 mysql 表在 DorisDB 中的名字，可以不同。

在 DorisDB 中创建 mysql 表的目的是可以通过 DorisDB 访问 mysql 数据库。而 DorisDB 本身并不维护、存储任何 mysql 数据。

（b）如果是 broker，表示表的访问需要通过指定的broker, 需要在 properties 提供以下信息：

~~~
PROPERTIES (
    "broker_name" = "broker_name",
    "path" = "file_path1[,file_path2]",
    "column_separator" = "value_separator"
    "line_delimiter" = "value_delimiter"
)
~~~

另外还需要提供Broker需要的Property信息，通过BROKER PROPERTIES来传递，例如HDFS需要传入：

~~~
BROKER PROPERTIES(
    "username" = "name",
    "password" = "password"
)
~~~

根据不同的Broker类型，需要传入的内容也不相同。

> 注："path" 中如果有多个文件，用逗号\[,\]分割。如果文件名中包含逗号，那么使用 %2c 来替代。如果文件名中包含 %，使用 %25 代替。现在文件内容格式支持CSV，支持GZ，BZ2，LZ4，LZO(LZOP) 压缩格式。

（c）如果是 hive，则需要在 properties 提供以下信息：

~~~
PROPERTIES (
    "database" = "hive_db_name",
    "table" = "hive_table_name",
    "hive.metastore.uris" = "thrift://127.0.0.1:9083"
)
~~~

其中 database 是 hive 表对应的库名字，table 是 hive 表的名字，hive.metastore.uris 是 hive metastore 服务地址。

> 注：目前hive外部表仅用于Spark Load使用，不支持查询。

  

**（4）key\_desc**

`语法：`

`   key_type(k1[,k2 ...])`

聚合类型。如果不指定

说明：

*   数据按照指定的key列进行排序，且根据不同的key\_type具有不同特性。
*   key\_type支持以下类型：

    *   AGGREGATE KEY：key列相同的记录，value列按照指定的聚合类型进行聚合， 适合报表、多维分析等业务场景。
    *   UNIQUE KEY：key列相同的记录，value列按导入顺序进行覆盖， 适合按key列进行增删改查的点查询业务。
    *   DUPLICATE KEY：key列相同的记录，同时存在于DorisDB中， 适合存储明细数据或者数据无聚合特性的业务场景。

默认为DUPLICATE KEY。key列为列定义中前36个字节, 如果前36个字节的列数小于3，将使用前三列。

> 注： 除AGGREGATE KEY外，其他key\_type在建表时，value列不需要指定聚合类型。

  

**（5）partition\_desc**

partition描述有两种使用方式：

<br>

（a）LESS THAN

语法：

~~~
PARTITION BY RANGE (k1, k2, ...)
(
PARTITION partition_name1 VALUES LESS THAN MAXVALUE|("value1", "value2", ...),
PARTITION partition_name2 VALUES LESS THAN MAXVALUE|("value1", "value2", ...),
...
)
~~~

说明：

使用指定的 key 列和指定的数值范围进行分区。

*   1) 分区名称仅支持字母开头，字母、数字和下划线组成；
*   2) 目前仅支持以下类型的列作为 Range 分区列，且只能指定一个分区列 TINYINT, SMALLINT, INT, BIGINT, LARGEINT, DATE, DATETIME；
*   3) 分区为左闭右开区间，首个分区的左边界为做最小值；
*   4) NULL 值只会存放在包含最小值的分区中。当包含最小值的分区被删除后，NULL 值将无法导入；
*   5) 可以指定一列或多列作为分区列。如果分区值缺省，则会默认填充最小值。

> 注：1) 分区一般用于时间维度的数据管理；2) 有数据回溯需求的，可以考虑首个分区为空分区，以便后续增加分区。

（b）Fixed Range

语法：

~~~
PARTITION BY RANGE (k1, k2, k3, ...) 
(
PARTITION partition_name1 VALUES [("k1-lower1", "k2-lower1", "k3-lower1",...), ("k1-upper1", "k2-upper1", "k3-upper1", ...)], 
PARTITION partition_name2 VALUES [("k1-lower1-2", "k2-lower1-2", ...), ("k1-upper1-2", MAXVALUE, "k3-upper1-2", ... )],
...
)
~~~

说明：

1）Fixed Range比LESS THAN相对灵活些，左右区间完全由用户自己确定；

2）其他与LESS THAN保持同步。

  

**（6）distribution\_desc**

当前仅支持 Hash 分桶的数据划分方式。

语法：DISTRIBUTED BY HASH (k1\[,k2 ...\]) \[BUCKETS num\]

说明：使用指定的 key 列进行哈希分桶。默认分区数为10。

  

**（7）PROPERTIES**

（a）如果 ENGINE 类型为 olap，可以在 properties 中设置该表数据的初始存储介质、存储到期时间和副本数。

~~~
PROPERTIES (
   "storage_medium" = "[SSD|HDD]",
   ["storage_cooldown_time" = "yyyy-MM-dd HH:mm:ss"],
   ["replication_num" = "3"]
)
~~~

*   storage\_medium：用于指定该分区的初始存储介质，可选择 SSD 或 HDD。默认初始存储介质可在 FE 的配置文件fe.conf中指定：default\_storage\_medium=xxx，如果没有指定，则默认为 HDD。

> 注意：当FE配置项 enable\_strict\_storage\_medium\_check 为 True 时，若集群中没有设置对应的存储介质时，建表语句会报错：Failed to find enough host in all backends with storage medium is SSD|HDD。

*   storage\_cooldown\_time：当设置存储介质为 SSD 时，指定该分区在 SSD 上的存储到期时间。格式为："yyyy-MM-dd HH:mm:ss"。默认存放 30 天。
*   replication\_num：指定分区的副本数。默认为 3。

  

当表为单分区表时，这些属性为表的属性。当表为两级分区时，这些属性附属于每一个分区。如果希望不同分区有不同属性，可以通过 ADD PARTITION 或 MODIFY PARTITION 进行操作。

  <br>

（b）如果 Engine 类型为 olap，可以指定某列使用 bloom filter 索引。bloom filter 索引仅适用于查询条件为 in 和 equal 的情况，该列的值越分散效果越好。目前只支持以下情况的列：除了 TINYINT、FLOAT、DOUBLE 类型以外的 key 列及聚合方法为 REPLACE 的 value 列。

~~~
PROPERTIES (
   "bloom_filter_columns"="k1,k2,k3"
)
~~~

（c）如果希望使用 Colocate Join 特性，需要在 properties 中指定：

~~~
PROPERTIES (
   "colocate_with"="table1"
)
~~~

  

（d）如果希望使用动态分区特性，需要在properties 中指定：

~~~
PROPERTIES (
    "dynamic_partition.enable" = "true|false",
    "dynamic_partition.time_unit" = "DAY|WEEK|MONTH",
    "dynamic_partition.start" = "${integer_value}",
    "dynamic_partitoin.end" = "${integer_value}",
    "dynamic_partition.prefix" = "${string_value}",
    "dynamic_partition.buckets" = "${integer_value}
)
~~~

*   dynamic\_partition.enable：用于指定表级别的动态分区功能是否开启。默认为 true。
*   dynamic\_partition.time\_unit：用于指定动态添加分区的时间单位，可选择为DAY（天）、WEEK(周)，或MONTH（月）。
*   dynamic\_partition.start：用于指定向前删除多少个分区。值必须小于0。默认为 Integer.MIN\_VALUE。
*   dynamic\_partition.end：用于指定提前创建的分区数量。值必须大于0。
*   dynamic\_partition.prefix：用于指定创建的分区名前缀。例如分区名前缀为p，则自动创建分区名为p20200108。
*   dynamic\_partition.buckets：用于指定自动创建的分区分桶数量。

  

（e）建表时可以批量创建多个 Rollup。

语法：

~~~
ROLLUP (rollup_name (column_name1, column_name2, ...)
    [FROM from_index_name]
    [PROPERTIES ("key"="value", ...)],...)
~~~

（f）如果希望使用内存表特性，需要在 properties 中指定：

~~~
PROPERTIES (
   "in_memory"="true"
)
~~~

当 in\_memory 属性为 true 时，DorisDB会尽可能将该表的数据和索引Cache到BE节点的内存中。

  

（g）创建UNIQUE\_KEYS表时，可以指定一个sequence列，当KEY列相同时，将按照sequence列进行REPLACE(较大值替换较小值，否则无法替换)。

~~~
PROPERTIES (
    "function_column.sequence_type" = 'Date',
)
~~~

sequence\_type用来指定sequence列的类型，可以为整型和时间类型。

  <br>

**示例**

（1）创建一个 olap 表，使用 HASH 分桶，使用列存，相同key的记录进行聚合。

~~~
CREATE TABLE example_db.table_hash (
    k1 TINYINT,
    k2 DECIMAL(10, 2) DEFAULT "10.5",
    v1 CHAR(10) REPLACE,
    v2 INT SUM
)
ENGINE=olap
AGGREGATE KEY(k1, k2)
COMMENT "my first doris table"
DISTRIBUTED BY HASH(k1) BUCKETS 32
PROPERTIES ("storage_type"="column");
~~~

（2）创建一个 olap 表，使用 Hash 分桶，使用列存，相同key的记录进行覆盖， 设置初始存储介质和冷却时间。

~~~
CREATE TABLE example_db.table_hash (
    k1 BIGINT,
    k2 LARGEINT,
    v1 VARCHAR(2048) REPLACE,
    v2 SMALLINT SUM DEFAULT "10"
)
ENGINE=olap
UNIQUE KEY(k1, k2)
DISTRIBUTED BY HASH (k1, k2) BUCKETS 32
PROPERTIES(
    "storage_type"="column"，
    "storage_medium" = "SSD",
    "storage_cooldown_time" = "2015-06-04 00:00:00"
);
~~~

  

（3）创建一个 olap 表，使用 Range 分区，使用Hash分桶，默认使用列存， 相同key的记录同时存在，设置初始存储介质和冷却时间。

（a）LESS THAN

~~~
CREATE TABLE example_db.table_range (
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    v1 VARCHAR(2048),
    v2 DATETIME DEFAULT "2014-02-04 15:36:00"
)
ENGINE=olap
DUPLICATE KEY(k1, k2, k3)
PARTITION BY RANGE (k1) (
    PARTITION p1 VALUES LESS THAN ("2014-01-01"),
    PARTITION p2 VALUES LESS THAN ("2014-06-01"),
    PARTITION p3 VALUES LESS THAN ("2014-12-01")
)
DISTRIBUTED BY HASH(k2) BUCKETS 32
PROPERTIES (
    "storage_medium" = "SSD", "storage_cooldown_time" = "2015-06-04 00:00:00"
);
~~~

这个语句会将数据划分成如下3个分区：

`( {    MIN     },   {"2014-01-01"} )`

`[ {"2014-01-01"},   {"2014-06-01"} )`

`[ {"2014-06-01"},   {"2014-12-01"} )`

不在这些分区范围内的数据将视为非法数据被过滤。

（b）Fixed Range

~~~
CREATE TABLE table_range (
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    v1 VARCHAR(2048),
    v2 DATETIME DEFAULT "2014-02-04 15:36:00"
)
ENGINE=olap
DUPLICATE KEY(k1, k2, k3)
PARTITION BY RANGE (k1, k2, k3) (
    PARTITION p1 VALUES [("2014-01-01", "10", "200"), ("2014-01-01", "20", "300")),
    PARTITION p2 VALUES [("2014-06-01", "100", "200"), ("2014-07-01", "100", "300"))
)
DISTRIBUTED BY HASH(k2) BUCKETS 32
PROPERTIES(
    "storage_medium" = "SSD"
);
~~~

  

（4）创建一个 mysql 表

~~~
CREATE EXTERNAL TABLE example_db.table_mysql (
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    k4 VARCHAR(2048),
    k5 DATETIME
)
ENGINE=mysql
PROPERTIES (
    "host" = "127.0.0.1",
    "port" = "8239",
    "user" = "mysql_user",
    "password" = "mysql_passwd",
    "database" = "mysql_db_test",
    "table" = "mysql_table_test"
);
~~~

  

（5）创建一个数据文件存储在HDFS上的 broker 外部表, 数据使用 "|" 分割，"\\n" 换行。

~~~
CREATE EXTERNAL TABLE example_db.table_broker (
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    k4 VARCHAR(2048),
    k5 DATETIME
)
ENGINE=broker
PROPERTIES (
    "broker_name" = "hdfs",
    "path" = "hdfs://hdfs_host:hdfs_port/data1,hdfs://hdfs_host:hdfs_port/data2,hdfs://hdfs_host:hdfs_port/data3%2c4",
    "column_separator" = "|",
    "line_delimiter" = "\n"
)
BROKER PROPERTIES (
    "username" = "hdfs_user",
    "password" = "hdfs_password"
);
~~~

  

（6）创建一张含有HLL列的表。

~~~
CREATE TABLE example_db.example_table (
    k1 TINYINT,
    k2 DECIMAL(10, 2) DEFAULT "10.5",
    v1 HLL HLL_UNION,
    v2 HLL HLL_UNION
)
ENGINE=olap
AGGREGATE KEY(k1, k2)
DISTRIBUTED BY HASH(k1) BUCKETS 32
PROPERTIES ("storage_type"="column");
~~~

（7）创建一张含有 BITMAP\_UNION 聚合类型的表（v1和v2列的原始数据类型必须是TINYINT,SMALLINT,INT）。

~~~
CREATE TABLE example_db.example_table (
    k1 TINYINT,
    k2 DECIMAL(10, 2) DEFAULT "10.5",
    v1 BITMAP BITMAP_UNION,
    v2 BITMAP BITMAP_UNION
)
ENGINE=olap
AGGREGATE KEY(k1, k2)
DISTRIBUTED BY HASH(k1) BUCKETS 32
PROPERTIES ("storage_type"="column");
~~~

（8）创建两张支持Colocat Join的表 t1 和 t2。

~~~
CREATE TABLE `t1` (
    `id` int(11) COMMENT "",
    `value` varchar(8) COMMENT ""
) ENGINE=OLAP
DUPLICATE KEY(`id`)
DISTRIBUTED BY HASH(`id`) BUCKETS 10
PROPERTIES (
    "colocate_with" = "t1"
);
CREATE TABLE `t2` (
    `id` int(11) COMMENT "",
    `value` varchar(8) COMMENT ""
) ENGINE=OLAP
DUPLICATE KEY(`id`)
DISTRIBUTED BY HASH(`id`) BUCKETS 10
PROPERTIES (
    "colocate_with" = "t1"
);
~~~

（9）创建一个数据文件存储在BOS上的 broker 外部表。

~~~
CREATE EXTERNAL TABLE example_db.table_broker (
    k1 DATE
)
ENGINE=broker
PROPERTIES (
    "broker_name" = "bos",
    "path" = "bos://my_bucket/input/file",
)
BROKER PROPERTIES (
    "bos_endpoint" = "http://bj.bcebos.com",
    "bos_accesskey" = "xxxxxxxxxxxxxxxxxxxxxxxxxx",
    "bos_secret_accesskey"="yyyyyyyyyyyyyyyyyyyy"
);
~~~

（10）创建一个带有 Bitmap 索引的表

~~~
CREATE TABLE example_db.table_hash (
    k1 TINYINT,
    k2 DECIMAL(10, 2) DEFAULT "10.5",
    v1 CHAR(10) REPLACE,
    v2 INT SUM,
    INDEX k1_idx (k1) USING BITMAP COMMENT 'xxxxxx'
)
ENGINE=olap
AGGREGATE KEY(k1, k2)
COMMENT "my first doris table"
DISTRIBUTED BY HASH(k1) BUCKETS 32
PROPERTIES ("storage_type"="column");
~~~

  

（11）创建一个动态分区表(需要在FE配置中开启动态分区功能)，该表每天提前创建3天的分区，并删除3天前的分区。例如今天为`2020-01-08`，则会创建分区名为`p20200108`, `p20200109`, `p20200110`, `p20200111`的分区。分区范围分别为：

`[types: [DATE]; keys: [2020-01-08]; ‥types: [DATE]; keys: [2020-01-09]; )`

`[types: [DATE]; keys: [2020-01-09]; ‥types: [DATE]; keys: [2020-01-10]; )`

`[types: [DATE]; keys: [2020-01-10]; ‥types: [DATE]; keys: [2020-01-11]; )`

`[types: [DATE]; keys: [2020-01-11]; ‥types: [DATE]; keys: [2020-01-12]; )`

  

~~~
CREATE TABLE example_db.dynamic_partition (
    k1 DATE,
    k2 INT,
    k3 SMALLINT,
    v1 VARCHAR(2048),
    v2 DATETIME DEFAULT "2014-02-04 15:36:00"
)
ENGINE=olap
DUPLICATE KEY(k1, k2, k3)
PARTITION BY RANGE (k1) (
    PARTITION p1 VALUES LESS THAN ("2014-01-01"),
    PARTITION p2 VALUES LESS THAN ("2014-06-01"),
    PARTITION p3 VALUES LESS THAN ("2014-12-01")
)
DISTRIBUTED BY HASH(k2) BUCKETS 32
PROPERTIES(
    "storage_medium" = "SSD",
    "dynamic_partition.time_unit" = "DAY",
    "dynamic_partition.start" = "-3",
    "dynamic_partition.end" = "3",
    "dynamic_partition.prefix" = "p",
    "dynamic_partition.buckets" = "32"
 );
~~~

  

（12）创建一个带 Rollup Index 的表

~~~
CREATE TABLE example_db.rolup_index_table (
    event_day DATE,
    siteid INT DEFAULT '10',
    citycode SMALLINT,
    username VARCHAR(32) DEFAULT '',
    pv BIGINT SUM DEFAULT '0'
)
AGGREGATE KEY(event_day, siteid, citycode, username)
DISTRIBUTED BY HASH(siteid) BUCKETS 10
rollup (
    r1(event_day,siteid),
    r2(event_day,citycode),
    r3(event_day)
)
PROPERTIES("replication_num" = "3");
~~~

  

（13）创建一个内存表

  

（14）创建一个hive外部表

~~~
CREATE TABLE example_db.table_hive (
    k1 TINYINT,
    k2 VARCHAR(50),
    v INT
)
ENGINE=hive
PROPERTIES (
    "database" = "hive_db_name",
    "table" = "hive_table_name",
    "hive.metastore.uris" = "thrift://127.0.0.1:9083"
);
~~~

#### 7.1.3.14 CREATE VIEW

**说明**

该语句用于创建一个逻辑视图。

`语法：`

`   CREATE VIEW [IF NOT EXISTS]`

`   [db_name.]view_name`

`(column1[ COMMENT "col comment"][, column2, ...])`

`   AS query_stmt`

`说明：`

`   1. 视图为逻辑视图，没有物理存储。所有在视图上的查询相当于在视图对应的子查询上进行。`

`   2. query_stmt 为任意支持的 SQL。`

  <br>

**示例**

（1）在 example\_db 上创建视图 example\_view。

`CREATE VIEW example_db.example_view (k1, k2, k3, v1)`

`AS`

`SELECT c1 as k1, k2, k3, SUM(v1) FROM example_table`

`WHERE k1 = 20160112 GROUP BY k1,k2,k3;`

（2）创建一个包含 comment 的 view。

`CREATE VIEW example_db.example_view (`

`k1 COMMENT "first key",`

`k2 COMMENT "second key",`

`k3 COMMENT "third key",`

`v1 COMMENT "first value"`

`)`

`COMMENT "my first view"`

`AS`

`SELECT c1 as k1, k2, k3, SUM(v1) FROM example_table`

`WHERE k1 = 20160112 GROUP BY k1,k2,k3;`



#### 7.1.3.15 CREATE FUNCTION

**说明**

此语句创建一个自定义函数。执行此命令需要用户拥有 ADMIN 权限。

`CREATE [AGGREGATE] FUNCTION function_name`

`   (arg_type [, ...])`

`   RETURNS ret_type`

`   [INTERMEDIATE inter_type]`

`   [PROPERTIES ("key" = "value" [, ...]) ]`

*   AGGREGATE：如果有此项，表示的是创建的函数是一个聚合函数，否则创建的是一个标量函数。
*   function\_name：要创建函数的名字, 可以包含数据库的名字。比如：db1.my\_func。
*   arg\_type：函数的参数类型，与建表时定义的类型一致。变长参数时可以使用, ...来表示。如果是变长类型，那么变长部分参数的类型与最后一个非变长参数类型一致。
*   ret\_type：函数返回类型。
*   inter\_type：用于表示聚合函数中间阶段的数据类型。
*   properties：用于设定此函数的相关属性，能够设置的属性包括：

*   "object\_file": 自定义函数动态库的URL路径，当前只支持 HTTP/HTTPS 协议，此路径需要在函数整个生命周期内保持有效。此选项为必选项。
*   "symbol": 标量函数的函数签名，用于从动态库里面找到函数入口。此选项对于标量函数是必选项。
*   "init\_fn": 聚合函数的初始化函数签名。对于聚合函数是必选项。
*   "update\_fn": 聚合函数的更新函数签名。对于聚合函数是必选项。
*   "merge\_fn": 聚合函数的合并函数签名。对于聚合函数是必选项。
*   "serialize\_fn": 聚合函数的序列化函数签名。对于聚合函数是可选项，如果没有指定，那么将会使用默认的序列化函数。
*   "finalize\_fn": 聚合函数获取最后结果的函数签名。对于聚合函数是可选项，如果没有指定，将会使用默认的获取结果函数。
*   "md5": 函数动态链接库的MD5值，用于校验下载的内容是否正确。此选项是可选项。
*   "prepare\_fn": 自定义函数的prepare函数的函数签名，用于从动态库里面找到prepare函数入口。此选项对于自定义函数是可选项。
*   "close\_fn": 自定义函数的close函数的函数签名，用于从动态库里面找到close函数入口。此选项对于自定义函数是可选项。

  

如果 function\_name 中包含了数据库名字，那么这个自定义函数会创建在对应的数据库中，否则这个函数将会创建在当前会话所在的数据库。新函数的名字与参数不能与当前命名空间中已存在的函数相同，否则会创建失败，但是只有名字相同、参数不同是能够创建成功的。

  

**示例**

（1）创建一个自定义标量函数

`CREATE FUNCTION my_add(INT, INT) RETURNS INT PROPERTIES (`

`   "symbol" =         "_ZN9doris_udf6AddUdfEPNS_15FunctionContextERKNS_6IntValES4_",`

`   "object_file" = "http://host:port/libmyadd.so"`

`);`

（2）创建一个有prepare/close函数的自定义标量函数

`CREATE FUNCTION my_add(INT, INT) RETURNS INT PROPERTIES (`

`   "symbol" =         "_ZN9doris_udf6AddUdfEPNS_15FunctionContextERKNS_6IntValES4_",`

`   "prepare_fn" = "_ZN9doris_udf14AddUdf_prepareEPNS_15FunctionContextENS0_18FunctionStateScopeE",`

`   "close_fn" = "_ZN9doris_udf12AddUdf_closeEPNS_15FunctionContextENS0_18FunctionStateScopeE",`

`   "object_file" = "http://host:port/libmyadd.so"`

`);`

（3）创建一个自定义聚合函数

`CREATE AGGREGATE FUNCTION my_count (BIGINT) RETURNS BIGINT PROPERTIES (`

`   "init_fn"="_ZN9doris_udf9CountInitEPNS_15FunctionContextEPNS_9BigIntValE",`

`   "update_fn"="_ZN9doris_udf11CountUpdateEPNS_15FunctionContextERKNS_6IntValEPNS_9BigIntValE",`

`   "merge_fn"="_ZN9doris_udf10CountMergeEPNS_15FunctionContextERKNS_9BigIntValEPS2_",`

`   "finalize_fn"="_ZN9doris_udf13CountFinalizeEPNS_15FunctionContextERKNS_9BigIntValE",`

`   "object_file"="http://host:port/libudasample.so"`

`);`

（4）创建一个变长参数的标量函数

`CREATE FUNCTION strconcat(varchar, ...) RETURNS varchar properties (`

`   "symbol" = "_ZN9doris_udf6StrConcatUdfEPNS_15FunctionContextERKNS_6IntValES4_",`

`   "object_file" = "http://host:port/libmyStrConcat.so"`

`);`

#### 7.1.3.16 DROP DATABASE

**说明**

该语句用于删除数据库（database）。

`语法：`

`   DROP DATABASE [IF EXISTS] db_name [FORCE];`

`说明：`

`1) 执行 DROP DATABASE 一段时间内，可以通过 RECOVER 语句恢复被删除的数据库。详见 RECOVER 语句。`

`2) 如果执行 DROP DATABASE FORCE，则系统不会检查该数据库是否存在未完成的事务，数据库将直接被删除并且不能被恢复，一般不建议执行此操作。`

  <br>

**示例**

删除数据库 db\_test。

`DROP DATABASE db_test;`

<br>

#### 7.1.3.17 DROP INDEX

**说明**

该语句用于从一个表中删除指定名称的索引，目前仅支持 Bitmap 索引。

`DROP INDEX index_name ON [db_name.]table_name;`

<br>

#### 7.1.3.18 DROP MATERIALIZED VIEW

**说明**

该语句用于删除物化视图。该操作是同步执行的。

`语法：`

`   DROP MATERIALIZED VIEW [IF EXISTS] mv_name ON table_name`

`说明：`

`   1) IF EXISTS：如果物化视图不存在，不抛出错误。如果不声明此关键字，物化视图不存在时报错。`

`   2) mv_name：待删除的物化视图的名称。必填项。`

`   3) table_name：待删除的物化视图所属的表名。必填项。`

  <br>

**示例**

示例表结构为：

`mysql> desc all_type_table all;`

`+----------------+-------+----------+------+-------+---------+-------+`

`| IndexName      | Field | Type     | Null | Key   | Default | Extra |`

`+----------------+-------+----------+------+-------+---------+-------+`

`| all_type_table | k1| TINYINT  | Yes| true | N/A |       |`

`|                | k2| SMALLINT | Yes| false | N/A | NONE  |`

`|                | k3| INT      | Yes| false | N/A | NONE  |`

`|                | k4| BIGINT   | Yes| false | N/A | NONE  |`

`|                | k5| LARGEINT | Yes| false | N/A | NONE  |`

`|                | k6| FLOAT    | Yes| false | N/A | NONE  |`

`|                | k7| DOUBLE   | Yes| false | N/A | NONE  |`

`|                |       |          |      |       |         |       |`

`| k1_sumk2       | k1| TINYINT  | Yes| true | N/A |       |`

`|                | k2| SMALLINT | Yes| false | N/A | SUM   |`

`+----------------+-------+----------+------+-------+---------+-------+`

  <br>

（1）删除表 all\_type\_table 的名为 k1\_sumk2 的物化视图

`drop materialized view k1_sumk2 on all_type_table;`

物化视图被删除后的表结构：

`+----------------+-------+----------+------+-------+---------+-------+`

`| IndexName      | Field | Type     | Null | Key   | Default | Extra |`

`+----------------+-------+----------+------+-------+---------+-------+`

`| all_type_table | k1| TINYINT  | Yes| true | N/A |       |`

`|                | k2| SMALLINT | Yes| false | N/A | NONE  |`

`|                | k3| INT      | Yes| false | N/A | NONE  |`

`|                | k4| BIGINT   | Yes| false | N/A | NONE  |`

`|                | k5| LARGEINT | Yes| false | N/A | NONE  |`

`|                | k6| FLOAT    | Yes| false | N/A | NONE  |`

`|                | k7| DOUBLE   | Yes| false | N/A | NONE  |`

`+----------------+-------+----------+------+-------+---------+-------+`

  <br>

（2）删除表 all\_type\_table 中一个不存在的物化视图

`drop materialized view k1_k2 on all_type_table;`

`ERROR 1064 (HY000): errCode = 2, detailMessage = Materialized view [k1_k2] does not exist in table [all_type_table]`

系统返回错误信息。

  <br>

（3）删除表 all\_type\_table 中的物化视图 k1\_k2，不存在时不报错。

`drop materialized view if exists k1_k2 on all_type_table;`

`Query OK, 0 rows affected (0.00 sec)`

<br>

#### 7.1.3.19 DROP REPOSITORY

**说明**

该语句用于删除一个已创建的仓库。仅 root 或 superuser 用户可以删除仓库。

`语法：`

`   DROP REPOSITORY `repo_name`;`

`说明：`

`   删除仓库，仅仅是删除该仓库在 Drois 中的映射，不会删除实际的仓库数据。删除后，可以再次通过指定相同的 broker 和 LOCATION 映射到该仓库。`

  <br>

**示例**

删除名为 bos\_repo 的仓库：

`DROP REPOSITORY `bos_repo`;`

  <br>

#### 7.1.3.20 DROP RESOURCE

**说明**

该语句用于删除一个已有的资源。仅 root 或 admin 用户可以删除资源。

`DROP RESOURCE 'resource_name';`

  

**示例**

删除名为 spark0 的 Spark 资源：

`DROP RESOURCE 'spark0';`

<br>

#### 7.1.3.21 DROP TABLE

**说明**

该语句用于删除 table 。

`语法：`

`   DROP TABLE [IF EXISTS] [FORCE] [db_name.]table_name ;`

`说明：`

`1) 执行 DROP TABLE 一段时间内，可以通过 RECOVER 语句恢复被删除的表。详见 RECOVER 语句。`

`2) 如果执行 DROP TABLE FORCE，则系统不会检查该表是否存在未完成的事务，表将直接被删除并且不能被恢复，一般不建议执行此操作。`

  <br>

**示例**

（1）删除一个 table

`DROP TABLE my_table;`

（2）如果存在，删除指定 database 的 table

`DROP TABLE IF EXISTS example_db.my_table;`

<br>

#### 7.1.3.22 DROP VIEW

**说明**

该语句用于删除一个逻辑视图 VIEW。

`语法：`

`   DROP VIEW [IF EXISTS] [db_name.]view_name;`

  

**示例**

如果存在，删除 example\_db 上的视图 example\_view。

`DROP VIEW IF EXISTS example_db.example_view;`

<br>

#### 7.1.3.23 DROP FUNCTION

**说明**

该语句用于删除一个自定义函数。当函数的名字、参数类型完全一致才能够被删除。

`DROP FUNCTION function_name (arg_type [, ...])`

*   function\_name：要删除函数的名字。
*   arg\_type：要删除函数的参数列表。

  

**示例**

删除函数 my\_add。

`DROP FUNCTION my_add(INT, INT)`

  <br>

#### 7.1.3.24 HLL

**说明**

HLL是基于HyperLogLog算法的工程实现，用于保存HyperLogLog计算过程的中间结果，它只能作为表的value列类型，通过聚合来不断的减少数据量，以此来实现加快查询的目的。基于它得到的是一个估算结果，误差大概在1%左右。

  <br>

HLL列是通过其它列或者导入数据里面的数据生成的。导入的时候通过hll\_hash函数来指定数据中哪一列用于生成HLL列。它常用于替代count distinct，通过结合 Rollup 在业务上用于快速计算UV等。

  <br>

相关函数：

`HLL_UNION_AGG(hll)`

`此函数为聚合函数，用于计算满足条件的所有数据的基数估算。此函数还可用于分析函数，只支持默认窗口，不支持window从句。`

`HLL_RAW_AGG(hll)`

`此函数为聚合函数，用于聚合hll类型字段，并且返回的还是hll类型。`

`HLL_CARDINALITY(hll)`

`此函数用于计算单条hll列的基数估算。`

`HLL_HASH(column_name)`

`生成HLL列类型，用于insert或导入的时候，导入的使用见相关说明。`

`EMPTY_HLL()`

`生成空HLL列，用于insert或导入的时候补充默认值，导入的使用见相关说明。`

  <br>

**示例**

（1）首先创建一张含有hll列的表

`create table test(`

`   dt date,`

`   id int,`

`   name char(10),`

`   province char(10),`

`   os char(1),`

`   set1 hll hll_union,`

`   set2 hll hll_union`

`)`

`distributed by hash(id) buckets 32;`

  <br>

（2）导入数据

（a）使用表中的列生成hll列

`curl --location-trusted -uname:password -T data -H "label:load_1" -H "columns:dt, id, name, province, os, set1=hll_hash(id), set2=hll_hash(name)"`

`   http://host/api/test_db/test/_stream_load`

（b）使用数据中的某一列生成hll列

`curl --location-trusted -uname:password -T data -H "label:load_1" -H "columns:dt, id, name, province, sex, cuid, os, set1=hll_hash(cuid), set2=hll_hash(os)"`

`   http://host/api/test_db/test/_stream_load`

  <br>

（3）聚合数据，常用方式3种（如果不聚合直接对base表查询，速度可能跟直接使用approx\_count\_distinct速度差不多）：

（a）创建一个rollup，让hll列产生聚合

`alter table test add rollup test_rollup(dt, set1);`

（b）创建另外一张专门计算UV的表，然后insert数据）

`create table test_uv(`

`   dt date,`

`   uv_set hll hll_union`

`)`

`distributed by hash(id) buckets 32;`

`insert into test_uv select dt, set1 from test;`

（c）创建另外一张专门计算UV的表，然后insert并通过hll\_hash根据test其它非hll列生成hll列

`create table test_uv(`

`   dt date,`

`   id_set hll hll_union`

`)`

`distributed by hash(id) buckets 32;`

`insert into test_uv select dt, hll_hash(id) from test;`

  <br>

（4）查询，hll列不允许直接查询它的原始值，可以通过配套的函数进行查询

（a）求总UV

`select HLL_UNION_AGG(uv_set) from test_uv;`

（b）求每一天的UV

`select dt, HLL_CARDINALITY(uv_set) from test_uv;`

（c）求test表中set1的聚合值

`select dt, HLL_CARDINALITY(uv) from (select dt, HLL_RAW_AGG(set1) as uv from test group by dt) tmp;`

`select dt, HLL_UNION_AGG(set1) as uv from test group by dt;`

  <br>

#### 7.1.3.25 RECOVER

**说明**

该语句用于恢复之前删除的 database、table 或者 partition。

`语法：`

`   1) 恢复 database`

`       RECOVER DATABASE db_name;`

`2) 恢复 table`

`RECOVER TABLE [db_name.]table_name;`

`   3) 恢复 partition`

`RECOVER PARTITION partition_name FROM [db_name.]table_name;`

`说明：`

`   1) 该操作仅能恢复之前一段时间内删除的元信息。默认为1天。（可通过fe.conf中catalog_trash_expire_second参数配置）。`

`   2) 如果删除元信息后新建立了同名同类型的元信息，则之前删除的元信息不能被恢复。`

  <br>

**示例**

（1）恢复名为 example\_db 的 database

`RECOVER DATABASE example_db;`

（2）恢复名为 example\_tbl 的 table

`RECOVER TABLE example_db.example_tbl;`

（3）恢复表 example\_tbl 中名为 p1 的 partition

`RECOVER PARTITION p1 FROM example_tbl;`

  <br>

#### 7.1.3.26 RESTORE

**说明**

该语句用于将之前通过 BACKUP 命令备份的数据恢复到指定数据库下。该命令为异步操作。提交成功后，需通过 SHOW RESTORE 命令查看进度。仅支持恢复 OLAP 类型的表。

`语法：`

`   RESTORE SNAPSHOT [db_name].{snapshot_name}`

`   FROM `repository_name``

`   ON (`

`       'table_name' [PARTITION ('p1', ...)] [AS 'tbl_alias'],`

`       ...`

`   )`

`   PROPERTIES ("key"="value", ...);`

`说明：`

`1) 同一数据库下只能有一个正在执行的 BACKUP 或 RESTORE 任务。`

`   2) ON 子句中标识需要恢复的表和分区。如果不指定分区，则默认恢复该表的所有分区。所指定的表和分区必须已存在于仓库备份中。`

`   3) 可以通过 AS 语句将仓库中备份的表名恢复为新的表，但新表名不能已存在于数据库中。分区名称不能修改。`

`   4) 可以将仓库中备份的表恢复替换数据库中已有的同名表，但须保证两张表的表结构完全一致。表结构包括：表名、列、分区、Rollup等等。`

`   5) 可以指定恢复表的部分分区，系统会检查分区 Range 是否能够匹配。`

`   6) PROPERTIES 目前支持以下属性：`

`   "backup_timestamp" = "2018-05-04-16-45-08"：指定了恢复对应备份的哪个时间版本，必填。该信息可以通过"SHOW SNAPSHOT ON repo;"语句获得。`

`   "replication_num" = "3"：指定恢复的表或分区的副本数。默认为3。若恢复已存在的表或分区，则副本数必须和已存在表或分区的副本数相同。同时，必须有足够的 host 容纳多个副本。`

`   "timeout" = "3600"：任务超时时间，默认为一天。单位秒。`

`   "meta_version" = 40：使用指定的 meta_version 来读取之前备份的元数据。注意，该参数作为临时方案，仅用于恢复老版本 DorisDB 备份的数据。最新版本的备份数据中已经包含 meta version，无需再指定。`

  

**示例**

（1）从 example\_repo 中恢复备份 snapshot\_1 中的表 backup\_tbl 到数据库 example\_db1，时间版本为 "2018-05-04-16-45-08"。恢复为 1 个副本。

`RESTORE SNAPSHOT example_db1.'snapshot_1'`

`FROM 'example_repo'`

`ON ( 'backup_tbl' )`

`PROPERTIES`

`(`

`   "backup_timestamp"="2018-05-04-16-45-08",`

`   "replication_num" = "1"`

`);`

  

（2）从 example\_repo 中恢复备份 snapshot\_2 中的表 backup\_tbl 的分区 p1、p2，以及表 backup\_tbl2 到数据库 example\_db1，并重命名为 new\_tbl，时间版本为 "2018-05-04-17-11-01"。默认恢复为 3 个副本。

`RESTORE SNAPSHOT example_db1.'snapshot_2'`

`FROM 'example_repo'`

`ON (`

`   'backup_tbl' PARTITION ('p1', 'p2'),`

`   'backup_tbl2' AS 'new_tbl'`

`)`

`PROPERTIES (`

`   "backup_timestamp"="2018-05-04-17-11-01"`

`);`

  <br>

#### 7.1.3.27 SHOW FUNCTIONS

**说明**

查看数据库下所有的自定义（或系统提供）的函数。如果指定了数据库，那么仅查看对应数据库中的函数，否则直接查询当前会话所在数据库中的函数。需要对被查看的数据库拥有SHOW权限。

`SHOW [FULL] [BUILTIN] FUNCTIONS [IN|FROM db] [LIKE 'function_pattern']`

*   full：表示显示函数的详细信息。
*   builtin：表示显示系统提供的函数。
*   db：要查询的数据库名字。
*   function\_pattern：用来过滤函数名称的参数。

  

**示例**

`mysql> show full functions in testDb\G`

`*************************** 1. row ***************************`

`       Signature: my_add(INT,INT)`

`Return Type: INT`

`Function Type: Scalar`

`Intermediate Type: NULL`

`      Properties: {"symbol":"_ZN9doris_udf6AddUdfEPNS_15FunctionContextERKNS_6IntValES4_","object_file":"http://host:port/libudfsample.so","md5":"cfe7a362d10f3aaf6c49974ee0f1f878"}`

`*************************** 2. row ***************************`

`       Signature: my_count(BIGINT)`

`Return Type: BIGINT`

`Function Type: Aggregate`

`Intermediate Type: NULL`

`      Properties: {"object_file":"http://host:port/libudasample.so","finalize_fn":"_ZN9doris_udf13CountFinalizeEPNS_15FunctionContextERKNS_9BigIntValE","init_fn":"_ZN9doris_udf9CountInitEPNS_15FunctionContextEPNS_9BigIntValE","merge_fn":"_ZN9doris_udf10CountMergeEPNS_15FunctionContextERKNS_9BigIntValEPS2_","md5":"37d185f80f95569e2676da3d5b5b9d2f","update_fn":"_ZN9doris_udf11CountUpdateEPNS_15FunctionContextERKNS_6IntValEPNS_9BigIntValE"}`

`2 rows in set (0.00 sec)`

`mysql> show builtin functions in testDb like 'year%';`

`+---------------+`

`| Function Name |`

`+---------------+`

`| year          |`

`| years_add     |`

`| years_diff    |`

`| years_sub     |`

`+---------------+`

`2 rows in set (0.00 sec)`

<br>

#### 7.1.3.28 SHOW RESOURCES

**说明**

该语句用于展示用户有使用权限的资源。普通用户仅能展示有使用权限的资源，root 或 admin 用户会展示所有的资源。

`语法`

`   SHOW RESOURCES`

`   [`

`       WHERE`

`       [NAME [ = "your_resource_name" | LIKE "name_matcher"]]`

`       [RESOURCETYPE = ["SPARK"]]`

`   ]`

`   [ORDER BY ...]`

`   [LIMIT limit][OFFSET offset];`

`说明：`

`   1) 如果使用 NAME LIKE，则会匹配 RESOURCES 的 Name 包含 name_matcher 的 Resource。`

`   2) 如果使用 NAME = ，则精确匹配指定的 Name。`

`   3) 如果指定了RESOURCETYPE，则匹配对应的 Resrouce 类型。`

`   4) 可以使用 ORDER BY 对任意列组合进行排序。`

`   5) 如果指定了 LIMIT，则显示 limit 条匹配记录，否则全部显示。`

`   6) 如果指定了 OFFSET，则从偏移量 offset 开始显示查询结果。默认情况下偏移量为0。`

  

**示例**

（1）展示当前用户拥有权限的所有Resource

`SHOW RESOURCES;`

（2）展示指定 Resource ，NAME 中包含字符串 "20140102"，展示10个属性

`SHOW RESOURCES WHERE NAME LIKE "2014_01_02" LIMIT 10;`

（3）展示指定 Resource ，指定 NAME 为 "20140102" 并按 KEY 降序排序

`SHOW RESOURCES WHERE NAME = "20140102" ORDER BY `KEY` DESC;`

<br>

#### 7.1.3.29 TRUNCATE TABLE

**说明**

该语句用于清空指定表和分区的数据。

`语法：`

`   TRUNCATE TABLE [db.]tbl[ PARTITION(p1, p2, ...)];`

`说明：`

`   1) 该语句清空数据，但保留表或分区。`

`2) 不同于 DELETE，该语句只能整体清空指定的表或分区，不能添加过滤条件。`

`3) 不同于 DELETE，使用该方式清空数据不会对查询性能造成影响。`

`   4) 该操作删除的数据不可恢复。`

`5) 使用该命令时，表状态需为 NORMAL，即不允许正在进行 SCHEMA CHANGE 等操作。`

  

**示例**

（1）清空 example\_db 下的表 tbl

`TRUNCATE TABLE example_db.tbl;`

（2）清空表 tbl 的 p1 和 p2 分区

`TRUNCATE TABLE tbl PARTITION(p1, p2);`

<br>

#### 7.1.3.30 CREATE TABLE LIKE
**说明**
该语句用于创建一个表结构和另一张表完全相同的空表。
1. 复制的表结构包括Column Defination、Partitions、Table Properties等
2. 用户需要对复制的原表有`SELECT`权限
3. 支持复制MySQL等外表

`语法：`
` CREATE [EXTERNAL] TABLE [IF NOT EXISTS] [database.]table_name LIKE [database.]table_name`

  <br>

**示例**
（1）在test1库下创建一张表结构和table1相同的空表，表名为table2
` CREATE TABLE test1.table2 LIKE test1.table1`
（2）在test2库下创建一张表结构和test1.table1相同的空表，表名为table2
` CREATE TABLE test2.table2 LIKE test1.table1`
（3）在test1库下创建一张表结构和MySQL外表table1相同的空表，表名为table2
` CREATE TABLE test1.table2 LIKE test1.table1`